{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c73a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8873f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file containing MRI image path and labels\n",
    "brain_df = pd.read_csv('Brain_MRI/data_mask.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b516335d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2556\n",
       "1    1373\n",
       "Name: mask, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 - No Tumor (2556 Images)\n",
    "# 1 - Tumor (1373 Images)\n",
    "brain_df['mask'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038ac23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3929, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the patient id column\n",
    "brain_df_train = brain_df.drop(columns = ['patient_id'])\n",
    "brain_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c515aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data in mask column to string format, to use categorical mode in flow_from_dataframe\n",
    "brain_df_train['mask'] = brain_df_train['mask'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd42616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>mask_path</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA_CS_5395_19981004/TCGA_CS_5395_19981004_1.tif</td>\n",
       "      <td>TCGA_CS_5395_19981004/TCGA_CS_5395_19981004_1_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA_CS_4944_20010208/TCGA_CS_4944_20010208_1.tif</td>\n",
       "      <td>TCGA_CS_4944_20010208/TCGA_CS_4944_20010208_1_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_1.tif</td>\n",
       "      <td>TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_1_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA_CS_4943_20000902/TCGA_CS_4943_20000902_1.tif</td>\n",
       "      <td>TCGA_CS_4943_20000902/TCGA_CS_4943_20000902_1_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA_CS_5396_20010302/TCGA_CS_5396_20010302_1.tif</td>\n",
       "      <td>TCGA_CS_5396_20010302/TCGA_CS_5396_20010302_1_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  TCGA_CS_5395_19981004/TCGA_CS_5395_19981004_1.tif   \n",
       "1  TCGA_CS_4944_20010208/TCGA_CS_4944_20010208_1.tif   \n",
       "2  TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_1.tif   \n",
       "3  TCGA_CS_4943_20000902/TCGA_CS_4943_20000902_1.tif   \n",
       "4  TCGA_CS_5396_20010302/TCGA_CS_5396_20010302_1.tif   \n",
       "\n",
       "                                           mask_path mask  \n",
       "0  TCGA_CS_5395_19981004/TCGA_CS_5395_19981004_1_...    0  \n",
       "1  TCGA_CS_4944_20010208/TCGA_CS_4944_20010208_1_...    0  \n",
       "2  TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_1_...    0  \n",
       "3  TCGA_CS_4943_20000902/TCGA_CS_4943_20000902_1_...    0  \n",
       "4  TCGA_CS_5396_20010302/TCGA_CS_5396_20010302_1_...    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9ccf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(brain_df_train, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "495088bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a image generator\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a data generator which scales the data from 0 to 1 and makes validation split of 0.15\n",
    "datagen = ImageDataGenerator(rescale=1./255., validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f9118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2839 validated image filenames belonging to 2 classes.\n",
      "Found 500 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator=datagen.flow_from_dataframe(dataframe=train,\n",
    "                                            directory= './Brain_MRI/',\n",
    "                                            x_col='image_path',\n",
    "                                            y_col='mask',\n",
    "                                            subset=\"training\",\n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            target_size=(128,128))\n",
    "\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(dataframe=train,\n",
    "                                            directory= './Brain_MRI/',\n",
    "                                            x_col='image_path',\n",
    "                                            y_col='mask',\n",
    "                                            subset=\"validation\",\n",
    "                                            batch_size=16,\n",
    "                                            shuffle=True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            target_size=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480e2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 590 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a data generator for test images\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(dataframe=test,\n",
    "                                                directory= './Brain_MRI/',\n",
    "                                                x_col='image_path',\n",
    "                                                y_col='mask',\n",
    "                                                batch_size=16,\n",
    "                                                class_mode='categorical',\n",
    "                                                target_size=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b67f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model (best saved one)\n",
    "with open('./Saved Model/classifier_model.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "    \n",
    "# load the model  \n",
    "model = tf.keras.models.model_from_json(json_savedModel)\n",
    "model.load_weights('./Saved Model/classifier_weights.h5')\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001), metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816a67d",
   "metadata": {},
   "source": [
    "## Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ca1993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 128, 128, 3)       3         \n",
      "_________________________________________________________________\n",
      "quant_conv2d (QuantizeWrappe (None, 128, 128, 32)      963       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 64, 64, 32)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_1 (QuantizeWrap (None, 64, 64, 64)        18627     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_1 (Quant (None, 32, 32, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_dropout (QuantizeWrapp (None, 32, 32, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_2 (QuantizeWrap (None, 32, 32, 64)        37059     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_2 (Quant (None, 16, 16, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_1 (QuantizeWra (None, 16, 16, 64)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_3 (QuantizeWrap (None, 16, 16, 64)        37059     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_3 (Quant (None, 8, 8, 64)          1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_4 (QuantizeWrap (None, 8, 8, 64)          37059     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_4 (Quant (None, 4, 4, 64)          1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_2 (QuantizeWra (None, 4, 4, 64)          1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_5 (QuantizeWrap (None, 4, 4, 64)          37059     \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_5 (Quant (None, 2, 2, 64)          1         \n",
      "_________________________________________________________________\n",
      "quant_dropout_3 (QuantizeWra (None, 2, 2, 64)          1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 256)               1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 64)                16453     \n",
      "_________________________________________________________________\n",
      "quant_dense_1 (QuantizeWrapp (None, 2)                 135       \n",
      "=================================================================\n",
      "Total params: 184,428\n",
      "Trainable params: 183,682\n",
      "Non-trainable params: 746\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b80508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "178/178 [==============================] - 33s 155ms/step - loss: 0.1012 - accuracy: 0.9718 - val_loss: 0.0280 - val_accuracy: 0.9940\n",
      "Epoch 2/5\n",
      "178/178 [==============================] - 25s 139ms/step - loss: 0.0900 - accuracy: 0.9711 - val_loss: 0.0282 - val_accuracy: 0.9960\n",
      "Epoch 3/5\n",
      "178/178 [==============================] - 25s 139ms/step - loss: 0.0736 - accuracy: 0.9768 - val_loss: 0.0279 - val_accuracy: 0.9920\n",
      "Epoch 4/5\n",
      "178/178 [==============================] - 25s 138ms/step - loss: 0.0681 - accuracy: 0.9785 - val_loss: 0.0257 - val_accuracy: 0.9920\n",
      "Epoch 5/5\n",
      "178/178 [==============================] - 24s 137ms/step - loss: 0.0685 - accuracy: 0.9775 - val_loss: 0.0251 - val_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "history = q_aware_model.fit(train_generator, validation_data= valid_generator, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "280a298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 3s 85ms/step - loss: 0.0403 - accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "scores = q_aware_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6221a405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 65). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kanak\\AppData\\Local\\Temp\\tmpnhol931o\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kanak\\AppData\\Local\\Temp\\tmpnhol931o\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_qaware_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68aeeff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size reduced from 2.29 MB to 196.98 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Model size reduced from 2.29 MB to\", np.around(len(tflite_qaware_model)/(1024),2), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a474ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brain_tumor_classifier.tflite\", 'wb') as f:\n",
    "    f.write(tflite_qaware_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
